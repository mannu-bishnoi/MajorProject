{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "884c49ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Concatenate\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17dadb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "1/1 [==============================] - 1s 748ms/step\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\photo\\src\\inpaint.cpp:768: error: (-210:Unsupported format or combination of formats) 8-bit, 16-bit unsigned or 32-bit float 1-channel and 8-bit 3-channel input/output images are supported in function 'icvInpaint'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 39>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m text_mask \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(text_mask, (image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Inpaint text regions with surrounding content\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m inpainted_image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minpaint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINPAINT_NS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Save the inpainted image\u001b[39;00m\n\u001b[0;32m     42\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.png\u001b[39m\u001b[38;5;124m'\u001b[39m, (inpainted_image \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8))\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\photo\\src\\inpaint.cpp:768: error: (-210:Unsupported format or combination of formats) 8-bit, 16-bit unsigned or 32-bit float 1-channel and 8-bit 3-channel input/output images are supported in function 'icvInpaint'\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained MobileNetV2 model without top (classification) layers\n",
    "base_model = MobileNetV2(input_shape=(256, 256, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "# Build the U-Net-like text removal model on top of MobileNetV2\n",
    "def build_text_removal_model(base_model):\n",
    "    # Decoder part (U-Net)\n",
    "    x = base_model.output\n",
    "    x = Conv2DTranspose(128, (3, 3), activation='relu', strides=(2, 2), padding='same')(x)\n",
    "    x = Concatenate()([x, base_model.get_layer('block_13_expand_relu').output])\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2DTranspose(64, (3, 3), activation='relu', strides=(2, 2), padding='same')(x)\n",
    "    x = Concatenate()([x, base_model.get_layer('block_6_expand_relu').output])\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2DTranspose(1, (3, 3), activation='sigmoid', strides=(2, 2), padding='same')(x)  # Output mask\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Build the text removal model\n",
    "model = build_text_removal_model(base_model)\n",
    "\n",
    "# Load the input image\n",
    "image = cv2.imread(\"C:\\MajorProject\\image_part_001 (22).png\")\n",
    "\n",
    "# Preprocess the image\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = cv2.resize(image, (256, 256))  # Resize to match the model's input size\n",
    "image = image / 255.0  # Normalize the pixel values to [0, 1]\n",
    "\n",
    "# Predict the text mask\n",
    "text_mask = model.predict(np.expand_dims(image, axis=0))[0]\n",
    "text_mask = (text_mask * 255).astype(np.uint8)  # Convert to 8-bit mask\n",
    "\n",
    "# Resize the text mask to match the input image size\n",
    "text_mask = cv2.resize(text_mask, (image.shape[1], image.shape[0]))\n",
    "\n",
    "# Inpaint text regions with surrounding content\n",
    "inpainted_image = cv2.inpaint(image, text_mask, 7, cv2.INPAINT_NS)\n",
    "\n",
    "# Save the inpainted image\n",
    "cv2.imwrite('output.png', (inpainted_image * 255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb2c897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
